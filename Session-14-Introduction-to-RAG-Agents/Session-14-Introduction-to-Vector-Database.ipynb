{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNL+kHfuj+D2J3D9jtaSYId"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### **To run this notebook efficiently you need to ensure**\n","1. You have GPU in your machine\n","2. Enable TPU on Colab\n","3. [Hugging Face token is setup in the notebook or environment](https://huggingface.co/settings/tokens)\n","4. Enable HF token for notebook scope.\n"],"metadata":{"id":"zNhIgpOlOLZP"}},{"cell_type":"markdown","metadata":{"id":"2ca89161"},"source":["## **Introduction to Vector Databases**\n"]},{"cell_type":"markdown","metadata":{"id":"72bf712d"},"source":["### What are Vector Databases?\n","\n","A Vector Database is a specialized database designed to store, manage, and search **high-dimensional vectors** (also known as embeddings) efficiently. Unlike traditional databases that store structured data, vector databases are optimized for handling numerical representations of data points, where each point (e.g., text, image, audio) is transformed into a vector of numbers that captures its semantic meaning or characteristics.\n","\n","#### Core Concepts:\n","\n","*   **Vector Embeddings**: These are numerical representations of data (like text, images, or audio) in a high-dimensional space. Machine learning models (e.g., neural networks) convert complex data into these vectors, where semantically similar items are located closer together in the vector space.\n","*   **Similarity Search**: This is the primary function of a vector database. It involves finding vectors that are 'similar' to a given query vector. Common metrics for measuring similarity include:\n","    *   **Cosine Similarity**: Measures the cosine of the angle between two vectors, indicating their directional similarity. Often used for text embeddings.\n","    *   **Euclidean Distance**: Measures the straight-line distance between two vectors in a multi-dimensional space. Shorter distances imply greater similarity.\n","*   **Approximate Nearest Neighbor (ANN) Algorithms**: Since exact nearest neighbor search in high-dimensional spaces is computationally intensive and impractical for large datasets, vector databases employ ANN algorithms (e.g., HNSW, IVFPQ, ANNOY). These algorithms allow for very fast similarity searches by sacrificing a small amount of accuracy for significant speed improvements.\n","\n","#### Essential for AI Applications:\n","\n","Vector databases are crucial for modern AI applications because they enable operations that are foundational to understanding and retrieving information based on meaning rather than exact keywords. Key applications include:\n","\n","*   **Semantic Search**: Allows users to search for content based on its meaning, even if the exact keywords are not present. For example, searching for \"pictures of playful cats\" and getting results for \"kittens frolicking\".\n","*   **Recommendation Systems**: By finding vectors similar to a user's preferences or previously liked items, vector databases can suggest relevant products, movies, or articles.\n","*   **Anomaly Detection**: Identifying data points (vectors) that are unusually distant from others, signaling potential fraud, errors, or rare events.\n","*   **Retrieval-Augmented Generation (RAG)**: This technique combines the power of large language models (LLMs) with external knowledge retrieval. A vector database stores a vast collection of documents as embeddings. When an LLM needs to answer a query, it first retrieves relevant contextual information from the vector database (based on semantic similarity) and then uses this information to generate a more accurate and informed response.\n","\n","By efficiently managing and searching these high-dimensional representations, vector databases bridge the gap between raw data and the semantic understanding required for advanced AI functionalities."]},{"cell_type":"markdown","metadata":{"id":"0fbf6571"},"source":["## **Vector vs. Traditional Databases**\n"]},{"cell_type":"markdown","metadata":{"id":"5386b859"},"source":["### Vector Databases vs. Traditional Databases\n","\n","Vector databases and traditional databases (relational and NoSQL) are designed for different purposes, leading to fundamental differences in their data structures, query types, and optimal use cases.\n","\n","#### **Data Structure**\n","\n","*   **Vector Databases**: Store data as high-dimensional vectors, which are numerical representations of objects (e.g., text, images, audio). These vectors capture the semantic meaning or features of the data. The underlying structure often involves specialized index structures (e.g., HNSW, Annoy, FAISS) to enable efficient similarity searches.\n","*   **Traditional Relational Databases (SQL)**: Organize data into tables with predefined schemas. Data is stored in rows and columns, enforcing strict relationships between different tables through keys. Data types are typically scalar (numbers, strings, dates).\n","*   **Traditional NoSQL Databases**: Offer more flexible schema designs. They can store data in various formats like documents (JSON/BSON), key-value pairs, wide-column stores, or graphs. While flexible, they primarily handle structured or semi-structured data, not high-dimensional vectors as their native data type.\n","\n","#### **Query Types**\n","\n","*   **Vector Databases**: Primarily designed for **similarity search** (also known as nearest neighbor search). Users query by providing a vector, and the database returns vectors that are semantically similar or"]},{"cell_type":"markdown","metadata":{"id":"4b543157"},"source":["### **Industry Leading Vector Databases**\n","\n","- **Pinecone**:\n","  A fully managed, cloud-native vector database designed for real-time applications at scale. Pinecone simplifies the deployment and management of vector search infrastructure, offering high performance, low latency, and a developer-friendly API. It's often used for semantic search, recommendation systems, and anomaly detection.\n","\n","- **Milvus**:\n","  An open-source vector database built for AI applications and similarity search. Milvus supports various vector indexes and provides efficient query performance for large-scale datasets. It can be deployed on-premise or in the cloud and is highly scalable. Its use cases include image recognition, video analysis, and drug discovery.\n","\n","- **Weaviate**:\n","  An open-source vector database that combines vector search with a GraphQL-based API for semantic search and knowledge graph capabilities. Weaviate is schema-aware, allowing for hybrid queries (vector and scalar filters) and offering a module system for integrating with different machine learning models and data sources. It is suitable for semantic search, recommendation engines, and chatbot applications.\n","\n","- **Qdrant**:\n","  An open-source vector similarity search engine and database, providing a production-ready service with a convenient API. Qdrant focuses on high-performance vector search with filtering capabilities, supporting various data types and deployments. It is known for its fast performance and suitability for applications like personalized recommendations, semantic search, and deduplication.\n","\n","- **ChromaDB**:\n","  An open-source vector database designed for ease of use, making it simple to build LLM applications. Chroma is lightweight and offers a straightforward API for embedding and querying documents. It focuses on being accessible for developers and is often used for RAG (Retrieval Augmented Generation) and other generative AI use cases."]},{"cell_type":"markdown","metadata":{"id":"b507dd77"},"source":["## **Setup Environment**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e8af1aea","executionInfo":{"status":"ok","timestamp":1769011704693,"user_tz":-330,"elapsed":2,"user":{"displayName":"Anant Prakash Awasthi","userId":"00676186995527977815"}},"outputId":"d5af9546-468f-4b17-86a1-e733461b37fa"},"source":["import torch\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"Is CUDA available: {torch.cuda.is_available()}\")"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch version: 2.9.0+cu126\n","Is CUDA available: True\n"]}]},{"cell_type":"markdown","metadata":{"id":"3c7c7f5f"},"source":["## **Import Text Data**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"87a7fe6f","executionInfo":{"status":"ok","timestamp":1769011704696,"user_tz":-330,"elapsed":2,"user":{"displayName":"Anant Prakash Awasthi","userId":"00676186995527977815"}},"outputId":"3ffeb93f-5436-407b-ad35-29a3fb26ae37"},"source":["example_text_data = \"\"\"\n","Vector databases are specialized databases designed to store, manage, and search high-dimensional vectors (also known as embeddings) efficiently. Unlike traditional databases that store structured data, vector databases are optimized for handling numerical representations of data points, where each point (e.g., text, image, audio) is transformed into a vector of numbers that captures its semantic meaning or characteristics.\n","\n","Core Concepts of Vector Databases:\n","1.  **Vector Embeddings**: These are numerical representations of data (like text, images, or audio) in a high-dimensional space. Machine learning models (e.g., neural networks) convert complex data into these vectors, where semantically similar items are located closer together in the vector space.\n","2.  **Similarity Search**: This is the primary function of a vector database. It involves finding vectors that are 'similar' to a given query vector. Common metrics for measuring similarity include Cosine Similarity (measures directional similarity) and Euclidean Distance (measures straight-line distance).\n","3.  **Approximate Nearest Neighbor (ANN) Algorithms**: Since exact nearest neighbor search in high-dimensional spaces is computationally intensive for large datasets, vector databases employ ANN algorithms (e.g., HNSW, IVFPQ, ANNOY). These algorithms allow for very fast similarity searches by sacrificing a small amount of accuracy for significant speed improvements.\n","\n","Essential for AI Applications:\n","Vector databases are crucial for modern AI applications because they enable operations foundational to understanding and retrieving information based on meaning rather than exact keywords. Key applications include:\n","\n","*   **Semantic Search**: Allows users to search for content based on its meaning, even if exact keywords are not present. For example, searching for \"pictures of playful cats\" and getting results for \"kittens frolicking\".\n","*   **Recommendation Systems**: By finding vectors similar to a user's preferences or previously liked items, vector databases can suggest relevant products, movies, or articles.\n","*   **Anomaly Detection**: Identifying data points (vectors) that are unusually distant from others, signaling potential fraud, errors, or rare events.\n","*   **Retrieval-Augmented Generation (RAG)**: This technique combines large language models (LLMs) with external knowledge retrieval. A vector database stores a vast collection of documents as embeddings. When an LLM needs to answer a query, it first retrieves relevant contextual information from the vector database (based on semantic similarity) and then uses this information to generate a more accurate and informed response.\n","\n","By efficiently managing and searching these high-dimensional representations, vector databases bridge the gap between raw data and the semantic understanding required for advanced AI functionalities.\n","\n","Traditional vs. Vector Databases:\n","Traditional relational databases (SQL) organize data into tables with predefined schemas, optimized for exact match queries, joins, and complex analytical queries. NoSQL databases offer flexible schema designs for various data formats like documents or key-value pairs, primarily handling structured or semi-structured data. In contrast, vector databases specifically store high-dimensional vectors for similarity searches. While traditional databases excel at managing structured, transactional, and analytical data, vector databases are purpose-built for semantic similarity searches, which are critical for modern AI-driven applications.\n","\n","Industry Leading Vector Databases:\n","-   **Pinecone**: A fully managed, cloud-native vector database designed for real-time applications at scale.\n","-   **Milvus**: An open-source vector database built for AI applications and similarity search, supporting various vector indexes.\n","-   **Weaviate**: An open-source vector database combining vector search with a GraphQL API for semantic search and knowledge graph capabilities.\n","-   **Qdrant**: An open-source vector similarity search engine and database, providing high-performance vector search with filtering capabilities.\n","-   **ChromaDB**: An open-source vector database designed for ease of use, making it simple to build LLM applications, often used for RAG.\n","\"\"\"\n","\n","print(example_text_data[:500]) # Print first 500 characters to confirm data load\n","print(f\"\\nTotal length of example_text_data: {len(example_text_data)} characters\")"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Vector databases are specialized databases designed to store, manage, and search high-dimensional vectors (also known as embeddings) efficiently. Unlike traditional databases that store structured data, vector databases are optimized for handling numerical representations of data points, where each point (e.g., text, image, audio) is transformed into a vector of numbers that captures its semantic meaning or characteristics.\n","\n","Core Concepts of Vector Databases:\n","1.  **Vector Embeddings**: These ar\n","\n","Total length of example_text_data: 4261 characters\n"]}]},{"cell_type":"markdown","metadata":{"id":"8e4044f2"},"source":["## **Text Chunking**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"059c7ec5","executionInfo":{"status":"ok","timestamp":1769011710103,"user_tz":-330,"elapsed":5407,"user":{"displayName":"Anant Prakash Awasthi","userId":"00676186995527977815"}},"outputId":"e73ba0f1-8840-42fd-8777-9d4c7a36cf2b"},"source":["!pip install langchain-community"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n","Requirement already satisfied: langchain-core<2.0.0,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.2.6)\n","Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.1)\n","Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.45)\n","Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n","Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n","Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n","Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n","Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.1)\n","Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n","Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n","Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.12.3)\n","Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\n","Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (25.0)\n","Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (4.15.0)\n","Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (0.13.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n","Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.5)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n","Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2026.1.4)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.12.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.41.4)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c7006e28","executionInfo":{"status":"ok","timestamp":1769011710111,"user_tz":-330,"elapsed":7,"user":{"displayName":"Anant Prakash Awasthi","userId":"00676186995527977815"}},"outputId":"d0ffdca7-fefa-4fe7-da6d-c9d7f5b5bf19"},"source":["from langchain_text_splitters import RecursiveCharacterTextSplitter\n","\n","# Initialize the splitter\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=500,\n","    chunk_overlap=50,\n","    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"] # Define separators for intelligent splitting\n",")\n","\n","# Split the text into chunks\n","text_chunks = text_splitter.split_text(example_text_data)\n","\n","# Print results\n","print(f\"Total number of chunks: {len(text_chunks)}\")\n","print(\"\\nFirst two chunks:\")\n","for i, chunk in enumerate(text_chunks[:2]):\n","    print(f\"--- Chunk {i+1} ---\")\n","    print(chunk)\n"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of chunks: 14\n","\n","First two chunks:\n","--- Chunk 1 ---\n","Vector databases are specialized databases designed to store, manage, and search high-dimensional vectors (also known as embeddings) efficiently. Unlike traditional databases that store structured data, vector databases are optimized for handling numerical representations of data points, where each point (e.g., text, image, audio) is transformed into a vector of numbers that captures its semantic meaning or characteristics.\n","--- Chunk 2 ---\n","Core Concepts of Vector Databases:\n","1.  **Vector Embeddings**: These are numerical representations of data (like text, images, or audio) in a high-dimensional space. Machine learning models (e.g., neural networks) convert complex data into these vectors, where semantically similar items are located closer together in the vector space.\n"]}]},{"cell_type":"markdown","metadata":{"id":"407a9a5e"},"source":["### Reasoning Behind Text Chunking\n","\n","Text chunking is a vital preprocessing step before generating embeddings for several reasons:\n","\n","*   **Input Limits of Embedding Models**: Most embedding models have a maximum token or character limit for their input. Large documents cannot be fed into these models directly. Chunking breaks down extensive texts into smaller, digestible segments that fit within these input constraints.\n","\n","*   **Granularity of Information (`chunk_size`)**: The `chunk_size` parameter defines the maximum length of each segment. Choosing an appropriate `chunk_size` is critical as it determines the granularity of the information that an embedding will represent. If chunks are too small, they might lack sufficient context. If they are too large, they might exceed model limits or combine too many disparate ideas, diluting the semantic meaning of the embedding.\n","\n","*   **Maintaining Context (`chunk_overlap`)**: The `chunk_overlap` parameter ensures that there is continuity and contextual flow between adjacent chunks. By having a small overlap, a chunk can include some of the preceding text, preventing the loss of meaning that might occur if a crucial sentence or idea is split exactly at a chunk boundary. This overlap helps to maintain semantic integrity when querying, as the retrieved chunks are more likely to contain complete thoughts or related concepts.\n","\n","*   **Intelligent Splitting (`RecursiveCharacterTextSplitter`)**: The `RecursiveCharacterTextSplitter` from `langchain` is designed to split text intelligently. Instead of arbitrarily cutting text at fixed intervals, it attempts to split based on a list of `separators` (like `\\n\\n`, `\\n`, `.`, ` `). It tries to split on larger, more semantically meaningful delimiters first (e.g., double newlines for paragraphs), and if a chunk is still too large, it progressively moves to smaller delimiters (e.g., single newlines, then periods, then spaces). This method helps ensure that chunks end on natural breaks in the text, preserving as much coherent meaning as possible within each chunk."]},{"cell_type":"markdown","metadata":{"id":"858854ff"},"source":["## **Generate Embeddings**\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9488559a","executionInfo":{"status":"ok","timestamp":1769011710124,"user_tz":-330,"elapsed":3,"user":{"displayName":"Anant Prakash Awasthi","userId":"00676186995527977815"}},"outputId":"b8c3bd5c-27af-4d38-975f-cb3677c31c5b"},"source":["from transformers import AutoTokenizer, AutoModel\n","import torch\n","\n","# Define the model name\n","model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n","\n","print(f\"Using model: {model_name} for embedding generation.\")"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Using model: sentence-transformers/all-MiniLM-L6-v2 for embedding generation.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"00a25341","executionInfo":{"status":"ok","timestamp":1769011712089,"user_tz":-330,"elapsed":1964,"user":{"displayName":"Anant Prakash Awasthi","userId":"00676186995527977815"}},"outputId":"93ec549a-8d5c-4c0a-ec36-d8f09a49ec12"},"source":["tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModel.from_pretrained(model_name)\n","\n","def get_embeddings(texts):\n","    # Tokenize the input texts\n","    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n","\n","    # Move model to CPU (Colab's free GPU tier might be limited for sentence-transformers in some cases, CPU is generally safer for smaller models without explicit device handling here).\n","    # For best performance on GPU, ensure model and inputs are on 'cuda' if available.\n","    with torch.no_grad():\n","        model_output = model(**encoded_input)\n","\n","    # Perform mean pooling to get sentence embeddings\n","    # (outputs.last_hidden_state * attention_mask.unsqueeze(-1)).sum(dim=1) / attention_mask.sum(dim=1).unsqueeze(-1)\n","    input_mask_expanded = encoded_input['attention_mask'].unsqueeze(-1).expand(model_output.last_hidden_state.size()).float()\n","    sum_embeddings = torch.sum(model_output.last_hidden_state * input_mask_expanded, 1)\n","    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","    embeddings = sum_embeddings / sum_mask\n","\n","    return embeddings.tolist()\n","\n","print(\"Tokenizer and model loaded. 'get_embeddings' function defined.\")"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenizer and model loaded. 'get_embeddings' function defined.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c1bdacaa","executionInfo":{"status":"ok","timestamp":1769011712855,"user_tz":-330,"elapsed":767,"user":{"displayName":"Anant Prakash Awasthi","userId":"00676186995527977815"}},"outputId":"50d370b1-e05a-4418-ddd3-bfef06e4e2e8"},"source":["embeddings = get_embeddings(text_chunks)\n","\n","print(f\"Total number of embeddings generated: {len(embeddings)}\")\n","if embeddings:\n","    print(f\"Shape of the first embedding: {len(embeddings[0])}\")"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of embeddings generated: 14\n","Shape of the first embedding: 384\n"]}]},{"cell_type":"markdown","metadata":{"id":"dd4709f5"},"source":["## **Save to ChromaDB**\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4cd39009","executionInfo":{"status":"ok","timestamp":1769011724207,"user_tz":-330,"elapsed":16,"user":{"displayName":"Anant Prakash Awasthi","userId":"00676186995527977815"}},"outputId":"4473ccf4-bd67-4e99-a2ad-bd8a57c03eb6"},"source":["import chromadb\n","\n","# 2. Initialize a ChromaDB client (in-memory for this demo)\n","client = chromadb.Client()\n","\n","# 3. Create a new collection\n","collection_name = 'my_document_collection'\n","collection = client.get_or_create_collection(name=collection_name)\n","\n","print(f\"ChromaDB client initialized and collection '{collection_name}' created.\")"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["ChromaDB client initialized and collection 'my_document_collection' created.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"30d52e1b","executionInfo":{"status":"ok","timestamp":1769011724219,"user_tz":-330,"elapsed":13,"user":{"displayName":"Anant Prakash Awasthi","userId":"00676186995527977815"}},"outputId":"d8f04952-648b-4df7-f658-3559944ad070"},"source":["import uuid\n","\n","# 4. Generate unique IDs for each text chunk\n","ids = [str(uuid.uuid4()) for _ in range(len(text_chunks))]\n","\n","# 5. Add the text_chunks, embeddings, and IDs to the ChromaDB collection\n","collection.add(\n","    embeddings=embeddings,\n","    documents=text_chunks,\n","    ids=ids\n",")\n","\n","# 6. Print a confirmation message\n","print(f\"Added {len(text_chunks)} documents (chunks) to ChromaDB collection '{collection_name}'.\")\n","print(f\"Collection count: {collection.count()} documents.\")"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Added 14 documents (chunks) to ChromaDB collection 'my_document_collection'.\n","Collection count: 14 documents.\n"]}]},{"cell_type":"markdown","metadata":{"id":"3ba89863"},"source":["## Summary:\n","\n","### Q&A\n","The task was to create a comprehensive tutorial on vector databases and implement a practical demonstration using ChromaDB. This was achieved by:\n","*   Explaining core concepts of vector databases, their differences from traditional databases, and listing industry leaders.\n","*   Implementing a practical demonstration involving setting up the environment, importing text data, chunking text, generating embeddings with a Qwen model (substituted with `sentence-transformers/all-MiniLM-L6-v2`), and storing chunks and embeddings in ChromaDB.\n","\n","### Data Analysis Key Findings\n","*   **Vector Database Concepts**: The tutorial defined vector databases as specialized systems for high-dimensional vectors (embeddings), emphasizing core concepts such as vector embeddings, similarity search (using metrics like Cosine Similarity and Euclidean Distance), and Approximate Nearest Neighbor (ANN) algorithms.\n","*   **AI Application Essentiality**: Vector databases were highlighted as crucial for AI applications like Semantic Search, Recommendation Systems, Anomaly Detection, and Retrieval-Augmented Generation (RAG).\n","*   **Vector vs. Traditional Databases**: Key distinctions were drawn based on:\n","    *   **Data Structure**: Vector databases store high-dimensional vectors for semantic meaning, while traditional relational databases use structured tables and NoSQL databases offer flexible schemas for structured/semi-structured data.\n","    *   **Query Types**: Vector databases are optimized for similarity search, whereas traditional databases excel at exact match queries (SQL) or various model-specific queries (NoSQL).\n","    *   **Use Cases**: Vector databases are suited for AI-driven semantic understanding, while traditional databases handle transactional data (SQL) or flexible, scalable data models (NoSQL).\n","*   **Industry Leaders**: Prominent vector databases like Pinecone, Milvus, Weaviate, Qdrant, and ChromaDB were described.\n","*   **Environment Setup**: Necessary libraries, including `chromadb`, `transformers`, `torch`, and `sentence-transformers`, were successfully installed.\n","*   **Text Data Preparation**: An example text dataset of 4261 characters explaining vector database concepts was loaded.\n","*   **Text Chunking**: The `example_text_data` was split into 14 manageable text chunks using `RecursiveCharacterTextSplitter` from `langchain_text_splitters` with a `chunk_size` of 500 characters and `chunk_overlap` of 50 characters, ensuring contextual continuity.\n","*   **Embedding Generation**: Embeddings were generated for all 14 text chunks using the `sentence-transformers/all-MiniLM-L6-v2` model. Each embedding had a dimension of 384.\n","*   **ChromaDB Storage**: An in-memory ChromaDB client was initialized, a collection named `my_document_collection` was created, and all 14 text chunks along with their generated embeddings and unique IDs were successfully added to this collection.\n","\n","### Insights or Next Steps\n","*   The populated ChromaDB collection is now ready for semantic search queries, allowing for retrieval of semantically similar text chunks based on a query embedding, which is foundational for RAG applications.\n","*   Further exploration could involve implementing a semantic search functionality to query the ChromaDB collection and retrieve relevant chunks, demonstrating the practical utility of the stored embeddings.\n"]}]}