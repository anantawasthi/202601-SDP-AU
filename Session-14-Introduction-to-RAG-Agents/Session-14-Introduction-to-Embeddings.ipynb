{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNDGW3KPAyqvgv8VTMxF2IF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### **To run this notebook efficiently you need to ensure**\n","1. You have GPU in your machine\n","2. Enable TPU on Colab\n","3. [Hugging Face token is setup in the notebook or environment](https://huggingface.co/settings/tokens)\n","4. Enable HF token for notebook scope.\n"],"metadata":{"id":"3cnF8egwOEIO"}},{"cell_type":"markdown","metadata":{"id":"f4ffaf03"},"source":["### **What are Embeddings?**\n","\n","Embeddings are numerical representations of discrete data (such as words, images, or entire documents) in a continuous vector space. Essentially, they transform qualitative, high-dimensional, and often sparse data into quantitative, low-dimensional, and dense vectors. Each dimension in the vector space captures some latent feature or characteristic of the original data point.\n","\n","### **Purpose of Embeddings in ML and NLP**\n","\n","The primary purpose of embeddings in machine learning and especially in Natural Language Processing (NLP) is to enable algorithms to process and understand complex data more effectively. By representing discrete entities as dense vectors, embeddings allow models to:\n","\n","1.  **Capture Relationships:** Similar items (e.g., words with similar meanings like 'king' and 'queen') are mapped to nearby points in the vector space, reflecting their semantic relationships.\n","2.  **Understand Context:** In NLP, word embeddings can capture contextual nuances, allowing models to differentiate between different meanings of the same word based on its surrounding words.\n","3.  **Improve Performance:** Traditional one-hot encoding for categorical data leads to high-dimensional, sparse vectors that can be computationally expensive and may suffer from the curse of dimensionality. Embeddings provide a much more efficient and meaningful representation.\n","4.  **Facilitate Transfer Learning:** Pre-trained embeddings can be reused across different tasks, reducing the need for large datasets and extensive training from scratch.\n","\n","### **Representing Data in a Dense Vector Space**\n","\n","Embeddings represent data in a dense, low-dimensional vector space. This means that each element (or dimension) in the vector typically has a non-zero value, unlike sparse representations (like one-hot encoding or Bag-of-Words) where most elements are zero. For example, a word embedding might be a vector of 300 floating-point numbers, where each number contributes to defining the word's position in the semantic space. This dense representation is crucial because it allows the embedding to capture a rich amount of semantic and syntactic information within a compact form. The proximity of vectors in this space often correlates with the similarity of the items they represent, making it easier for machine learning models to identify patterns, classify data, and make predictions based on these inherent relationships."]},{"cell_type":"markdown","metadata":{"id":"bb195fd0"},"source":["## **Overview of Embedding Strategies**\n","\n","### **Statistical Embedding Methods**\n","Statistical embedding methods generate fixed-size vector representations (embeddings) for words or phrases based on their statistical properties in a large corpus. These methods typically learn a single, context-independent vector for each word.\n","\n","*   **Word2Vec**: This model uses neural networks (either Skip-gram or CBOW architectures) to learn word associations from a large corpus of text. Words that appear in similar contexts will have similar vector representations.\n","*   **GloVe (Global Vectors for Word Representation)**: GloVe is an unsupervised learning algorithm for obtaining vector representations for words. It builds upon the idea of word-context co-occurrence matrices, combining global matrix factorization and local context window methods. It captures global corpus statistics by training on the non-zero entries of a word-word co-occurrence matrix.\n","\n","**General Characteristics**: These methods typically produce static embeddings, meaning each word has one fixed vector representation regardless of its context in a sentence. They rely heavily on word co-occurrence statistics to capture semantic relationships.\n","\n","### **Contextual Embedding Methods**\n","Contextual embedding methods generate word embeddings that are dynamic and vary based on the specific context in which a word appears within a sentence. These methods leverage transformer architectures to understand complex linguistic nuances.\n","\n","*   **BERT (Bidirectional Encoder Representations from Transformers)**: BERT is a pre-trained deep learning model that processes words in relation to all other words in a sentence, both to its left and right (bidirectionally). This allows it to generate context-dependent embeddings, meaning the embedding for a word like 'bank' will differ depending on whether it refers to a financial institution or a river bank.\n","*   **Sentence-BERT**: An extension of BERT, Sentence-BERT (SBERT) fine-tunes pre-trained BERT, RoBERTa, or other transformer networks to produce semantically meaningful sentence embeddings. It modifies the architecture to yield fixed-size sentence embeddings that can be compared directly using cosine similarity, making it highly effective for tasks like semantic search and clustering.\n","\n","**General Characteristics**: These methods produce dynamic embeddings, where a word's vector representation changes based on its surrounding words in a sentence. They excel at capturing polysemy and more complex semantic relationships, leading to improved performance in many natural language processing tasks."]}]}